lines(x1, f_hat[i,,3], col = rgb(0,0,0, 0.2))
}
plot(NULL, xlim = c(-2, 2), ylim = c(-2, 2))
for(i in 1: 50){
lines(x1, f_hat[i,,5], col = rgb(0,0,0, 0.2))
}
par(op)
set.seed(123)
n <- 20
a <- 0
b1 <- 0.15
b2 <- -0.4
s <- 1
nrep <- 1000
dev_in  <- matrix(NA, nrep, 5)
dev_out <- matrix(NA, nrep, 5)
for(i in 1: nrep){
X_in  <- matrix(runif(n*4, -3, 3), n, 4)
X_out <- matrix(runif(n*4, -3, 3), n, 4)
y_in  <- rnorm(n, mean = a + b1 * X_in[,1] + b2 * X_in[,2], sd = s)
y_out <- rnorm(n, mean = a + b1 * X_out[,1] + b2 * X_out[,2], sd = s)
m1 <- lm(y_in ~ 1)
m2 <- lm(y_in ~ X_in[,1])
m3 <- lm(y_in ~ X_in[,1:2])
m4 <- lm(y_in ~ X_in[,1:3])
m5 <- lm(y_in ~ X_in[,1:4])
dev_in[i, 1] <- -2 * sum(dnorm(y_in, coef(m1), sd = s, log = TRUE))
dev_in[i, 2] <- -2 * sum(dnorm(y_in, cbind(numeric(n)+1, X_in[,1]) %*% coef(m2), sd = s, log = TRUE))
dev_in[i, 3] <- -2 * sum(dnorm(y_in, cbind(numeric(n)+1, X_in[,1:2]) %*% coef(m3), sd = s, log = TRUE))
dev_in[i, 4] <- -2 * sum(dnorm(y_in, cbind(numeric(n)+1, X_in[,1:3]) %*% coef(m4), sd = s, log = TRUE))
dev_in[i, 5] <- -2 * sum(dnorm(y_in, cbind(numeric(n)+1, X_in[,1:4]) %*% coef(m5), sd = s, log = TRUE))
dev_out[i, 1] <- -2 * sum(dnorm(y_out, coef(m1), sd = s, log = TRUE))
dev_out[i, 2] <- -2 * sum(dnorm(y_out, cbind(numeric(n)+1, X_out[,1]) %*% coef(m2), sd = s, log = TRUE))
dev_out[i, 3] <- -2 * sum(dnorm(y_out, cbind(numeric(n)+1, X_out[,1:2]) %*% coef(m3), sd = s, log = TRUE))
dev_out[i, 4] <- -2 * sum(dnorm(y_out, cbind(numeric(n)+1, X_out[,1:3]) %*% coef(m4), sd = s, log = TRUE))
dev_out[i, 5] <- -2 * sum(dnorm(y_out, cbind(numeric(n)+1, X_out[,1:4]) %*% coef(m5), sd = s, log = TRUE))
}
xs = 1:5
sd_in <- numeric(5)
for(i in 1:5) sd_in[i] <- sd(dev_in[,i])
sd_out <- numeric(5)
for(i in 1:5) sd_out[i] <- sd(dev_out[,i])
plot(xs, colMeans(dev_in), type = "o", xlim = c(1,5.5), ylim = c(40,90), xlab = "Complexity", ylab = "Deviance")
lines(xs+0.2, colMeans(dev_out), type = "o", pch = 19)
segments(xs, colMeans(dev_in)-sd_in, xs, colMeans(dev_in)+sd_in)
segments(xs+0.2, colMeans(dev_out)-sd_out, xs+0.2, colMeans(dev_out)+sd_out)
# Specify uncertainty in the parameters:
r = 1          ## Intrinsic growth rate
K = 10         ## carrying capacity
r.sd = 0.2     ## standard deviation on r
K.sd = 1.0     ## standard deviation on K
# Simulation parameters:
NE = 1000      ## Ensemble size
NT = 200       ## Time steps in each run
n0 = 0.1         ## Initial population size
# Now run the logistic growth model 1000 times, each time with slightly
# different parameters. Store all 1000 trajectories in a matrix.
n = matrix(n0,NE,NT)   # storage for all simulations
rE = rnorm(NE,r,r.sd)  # sample values of r
KE = rnorm(NE,K,K.sd)  # sample values of K
for(i in 1:NE){        # loop over r and K samples
for(t in 2:NT){      # for each sample, simulate throught time
n[i,t] = n[i,t-1] + rE[i]*n[i,t-1]*(1-n[i,t-1]/KE[i])
}
}
# calculate the median and 95% CI limits for each time point
n.stats = apply(n,2,quantile,c(0.025,0.5,0.975))
# plot 50 runs from the ensemble
matplot(1:NT,t(n[1:50,]),type="l",col="black",lty=1)
# plot the median
plot(1:NT,n.stats[2,],type="l",
ylim=c(0,2*K))  # leave room on y axis for CIs
# add upper and lower CI's
lines(1:NT,n.stats[1,],col="blue",lty="dashed")
lines(1:NT,n.stats[3,], col="blue",lty="dashed")
library(mvtnorm)
my_mu = c(1,10)
my_sigma = cbind(c(1,0),c(0,5))  # no correlations
print(my_sigma)
out = rmvnorm(100,mean=my_mu,sigma=my_sigma)
plot(out)
my_sigma = cbind(c(1,2),c(2,5))  # positive correlations
print(my_sigma)
out = rmvnorm(100,mean=my_mu,sigma=my_sigma)
plot(out)
my_sigma = cbind(c(1,-2),c(-2,5))  # negative correlations
out = rmvnorm(100,mean=my_mu,sigma=my_sigma)
plot(out)
# simulate an independent variable and a response
x = seq(1,100,5)
y = 0.8*x + rnorm(length(x),0,max(x)/5)  # rnorm() adds some noise
plot(x,y)
reg = lm(y~x)
# extract estimates of parameter means and var-cov matrix
mu = coef(reg)
sigma = vcov(reg)
# Monte Carlo simulations
NE = 100      # Ensemble size
n = matrix(NA,NE,length(x))   # storage for all simulations
coef_samples = rmvnorm(NE,mean=mu,sigma=sigma)  # sample values of coefficients
for(i in 1:NE){        # loop over coefficient samples
n[i,] = coef_samples[i,1] + coef_samples[i,2]*x   # predict values at each x
}
# calculate the median and 95% CI limits for each time point
n.stats = apply(n,2,quantile,c(0.025,0.5,0.975))
# plot the data
plot(x,y)
# add predictions, upper and lower CI's
lines(x,predict(reg))
lines(x,n.stats[1,],col="blue",lty="dashed")
lines(x,n.stats[3,], col="blue",lty="dashed")
# Specify uncertainty in the parameters:
r = 1          ## Intrinsic growth rate
K = 10         ## carrying capacity
r.sd = 0.2     ## standard deviation on r
K.sd = 1.0     ## standard deviation on K
# Simulation parameters:
NE = 1000      ## Ensemble size
NT = 200       ## Time steps in each run
n0 = 0.1         ## Initial population size
# Now run the logistic growth model 1000 times, each time with slightly
# different parameters. Store all 1000 trajectories in a matrix.
n = matrix(n0,NE,NT)   # storage for all simulations
rE = rnorm(NE,r,r.sd)  # sample values of r
KE = rnorm(NE,K,K.sd)  # sample values of K
for(i in 1:NE){        # loop over r and K samples
for(t in 2:NT){      # for each sample, simulate throught time
n[i,t] = n[i,t-1] + rE[i]*n[i,t-1]*(1-n[i,t-1]/KE[i])
}
}
# calculate the median and 95% CI limits for each time point
n.stats = apply(n,2,quantile,c(0.025,0.5,0.975))
?apply
dim(n.stats)
matplot(1:NT,t(n[1:50,]),type="l",col="black",lty=1)
# plot the median
plot(1:NT,n.stats[2,],type="l",
ylim=c(0,2*K))  # leave room on y axis for CIs
# add upper and lower CI's
lines(1:NT,n.stats[1,],col="blue",lty="dashed")
lines(1:NT,n.stats[3,], col="blue",lty="dashed")
1:10
rep(c(1,2,3),2)
for(my_index in rep(c(1,2,3),2)){
paste("my_index=",my_index)
}
for(my_index in rep(c(1,2,3),2)){
paste("my_index=",my_index)
flush.console()
}
for(my_index in rep(c(1,2,3),2)){
print(paste("my_index=",my_index))
}
my_vector=c("A","B","C")
for(my_index in my_vector){
print(paste("my_index=",my_vector))
}
for(my_index in 1:10){
print(paste("my_index=",))
}
for(my_index in 1:10){
print(paste("my_index=",my_index))
}
for(my_index in 1:10){
print(paste("my_index=",my_index^2))
}
A = c("A","B","C")
for(my_index in 1:3){
print(paste("my_index=",A[my_index]))
}
my_matrix=c(NA,3,3)
matrix
my_matrix
my_matrix=matrix(NA,3,3)
my_matrix
my_matrix=matrix(NA,3,3)
for(icol in 1:3){
for(irow in 1:3){
my_matrix[irow,icol] <- icol*irow
}
}
my_matrix
my_matrix=matrix(NA,3,3)
counter=0
for(icol in 1:3){
for(irow in 1:3){
counter = counter + 1
my_matrix[irow,icol] <- counter
}
}
my_matrix
my_matrix=matrix(NA,3,3)
counter=0
for(icol in 1:3){
for(irow in 1:3){
print(paste0("row=",irow,"; col=",icol))
}
}
# conduct individual level analysis of climate and competition
# effects on cheatgrass fitness in satellite experiments
# start clean
rm(list=ls())
# To use relative paths, we need to set working directory to source file location
# (this method only works on Rstudio)
library(rstudioapi)
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path )) # set working directory to location of this file
# load packages
library(dplyr)
library(tidyr)
library(maps)
# load and clean 2021, 2022, and 2023 demography data
source("load_and_clean_demography_data.R")
# pull Daymet data for each site, multiple years
# this takes a few minutes, no need to run more than once
# source("pull_climate_data.R")
# load site info and site climate data
source("load_site_climate_data.R")
# load and clean composition data, add functional group info
source("composition2functional_groups.R")
names(D)
names(comp_ftypes1)
# conduct individual level analysis of climate and competition
# effects on cheatgrass fitness in satellite experiments
# start clean
rm(list=ls())
# To use relative paths, we need to set working directory to source file location
# (this method only works on Rstudio)
library(rstudioapi)
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path )) # set working directory to location of this file
# load packages
library(dplyr)
library(tidyr)
library(maps)
# load and clean 2021, 2022, and 2023 demography data
source("load_and_clean_demography_data.R")
# pull Daymet data for each site, multiple years
# this takes a few minutes, no need to run more than once
# source("pull_climate_data.R")
# load site info and site climate data
source("load_site_climate_data.R")
# This code
# 1. cleans up and combines compositional data from all years into one composition
# file written to deriveddata,
# 2. compiles and cleans species names for Bromus tectorum "competitors" and
# 3. joins functional type data to species names
# To use relative paths, we need to set working directory to source file location
# (this method only works on Rstudio)
library(rstudioapi)
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path )) # set working directory to location of this file
# Load libraries
library(tidyverse)
# Make %notin% operator
`%notin%` <- Negate(`%in%`)
# Read in 2020-2021 composition data
comp20 <- read_csv("../rawdata/Satellite_composition_2020-2021.csv")
# Read in 2021-2022 composition data
comp21 <- read_csv("../rawdata/Satellite_composition_2021-2022.csv")
# Read in 2020-2021 composition data
comp22 <- read_csv("../rawdata/Satellite_composition_2022-2023.csv")
# Rename columns for brevity
comp20 %>% mutate(year=2021) %>%
select(sitecode = SiteCode,
year=year,
transect = `Transect (N, E, S, or W)`,
treatment = `Treatment (control OR removal)`,
distance_m = `Distance from center (m)`,
species = Species,
cover = Cover) -> comp20
#litter_depth_cm = `Litter depth (if >1cm)`,
#notes = Notes) -> comp20
#add missing columns
comp20$litter_depth_cm <- NA
comp20$notes <- NA
comp21 %>% mutate(year=2022) %>%
select(sitecode = SiteCode,
year=year,
transect = `Transect (N, E, S, or W)`,
treatment = `Treatment (control OR removal)`,
distance_m = `Distance from center (m)`,
species = Species,
cover = Cover,
litter_depth_cm = `Litter depth (cm)`,
notes = Notes) -> comp21
comp22 %>% mutate(year=2023) %>%
select(sitecode = SiteCode,
year=year,
transect = `Transect (N, E, S, or W)`,
treatment = `Treatment (control OR removal)`,
distance_m = `Distance from center (m)`,
species = Species,
cover = Cover,
litter_depth_cm = `Litter depth (if >1cm)`,
notes = Notes) -> comp22
# Combine species observation lists
comp_all <- rbind(comp20, comp21, comp22)
comp_all <- comp_all[order(comp_all$sitecode,comp_all$year,comp_all$transect,comp_all$distance_m),]
rm(comp20,comp21,comp22)
# PBA: I'm ignoring the problematic "notes" because I cleaned the demography
# data very carefully. That catches the real problems. Leftover notes for composition
# mostly concern species ID, which don't matter much at functional group level.
# Write csv file to code species as functional groups manually and to fix any issues
species_list <- sort(unique(comp_all$species))
write_csv(tibble(species = species_list), "../deriveddata/species_list_raw.csv" )
# did a previous species list exist?
tmp <- file.exists("../deriveddata/species_list_updates.csv")
# if so, add any new entries to that list
if(tmp==T){
update_list <- read.csv("../deriveddata/species_list_updates.csv")
update_list <- update_list[,c("species","update")]
tmp <- species_list %notin% update_list$species
if(sum(tmp)>0){
tmp <- data.frame("species" = species_list[tmp], "update" = NA)
update_list <- rbind(update_list,tmp)
update_list <- update_list[order(update_list$species),]
#write to file
write.csv(update_list,"../deriveddata/species_list_updates.csv",row.names=F)
}
}
# DONE BY HAND: fill in "update" column in
# "../deriveddata/species_list_updates.csv" as needed
# # look up sitecode for a species code
# findspp <- "ERSP"
# comp_all[comp_all$species==findspp,]
# Read in species updates
updated_names <- read.csv("../deriveddata/species_list_updates.csv",header=T)
updated_names <- updated_names[,c("species","update")] #drop site column
# fill missing values with NA
tmp <- which(updated_names$update=="")
updated_names$update[tmp] <- NA
# update species names
comp_all <- merge(comp_all, updated_names, all.x = T)
tmp <- which(!is.na(comp_all$update))
comp_all$species[tmp] <- comp_all$update[tmp]
comp_all <- dplyr::select(comp_all,-update) # drop update column
# write updated species list (to join with functional trait data)
species_list_clean <- data.frame(species=sort(unique(comp_all$species)))
write.csv(species_list_clean,"../deriveddata/species_list_clean.csv",row.names=F)
# clean up
rm(tmp,update_list,updated_names,species_list_clean)
###
# join species list with functional type data
spp_list <- read.csv("../deriveddata/species_list_clean.csv",header=T)
fgroups <- read.csv("../deriveddata/species2functionalgroups.csv",header=T)
spp_list <- merge(spp_list,fgroups, all.x=T)
write.csv(spp_list,"../deriveddata/species2functionalgroups.csv", row.names=F)
rm(spp_list)
# fill out functional type data by hand
# read in latest version of functional type data
fgroups <- read.csv("../deriveddata/species2functionalgroups.csv",header=T)
### define functional groups
# annual, perennial, shrub, biocrust, unknown
fgroups$ftypes1 <- NA
fgroups$ftypes1[fgroups$duration=="annual" | fgroups$duration=="biennial"] <- "annual"
fgroups$ftypes1[fgroups$duration=="perennial"] <- "perennial"
fgroups$ftypes1[fgroups$duration=="unknown"] <- "unknown"
fgroups$ftypes1[fgroups$type=="shrub"] <- "shrub"
fgroups$ftypes1[fgroups$type=="biocrust"] <- "biocrust"
# unknowns and non-plants are NAs, these will be CUT
# annual forb, annual grass, perennial forb, perennial grass, shrub, biocrust
fgroups$ftypes2 <- fgroups$ftypes1
tmp <- which(fgroups$ftypes1=="annual")
fgroups$ftypes2[tmp] <- paste0(fgroups$ftypes2[tmp],fgroups$type[tmp])
tmp <- which(fgroups$ftypes1=="perennial")
fgroups$ftypes2[tmp] <- paste0(fgroups$ftypes2[tmp],fgroups$type[tmp])
# annual forb, annual grass, perennial forb, perennial grass c3,  perennial grass c4, shrub, biocrust
fgroups$ftypes3 <- fgroups$ftypes2
tmp <- which(fgroups$ftypes2=="perennialgrass")
fgroups$ftypes3[tmp] <- paste0(fgroups$ftypes3[tmp],fgroups$c3c4[tmp])
fgroups <- fgroups[,c("species","ftypes1","ftypes2","ftypes3")]
### join functional group data to composition data and do some cleaning
comp_all <- merge(comp_all,fgroups,all.x=T)
# remove records where cover == NA. These are missing toothpicks and 2 bare ground obs
tmp <- which(is.na(comp_all$cover))
comp_all <- comp_all[-tmp,]
# remove other missing records flagged by value of cover = -1
tmp <- which(comp_all$cover == -1)
comp_all <- comp_all[-tmp,]
tmp <- which(comp_all$cover == "missing")
comp_all <- comp_all[-tmp,]
tmp <- which(comp_all$cover == "M")
comp_all <- comp_all[-tmp,]
# other checks
table(comp_all$ftypes1)
table(comp_all$cover)
# fix non-numeric cover values
comp_all$cover[comp_all$cover == "<1"] <- 0.5
comp_all$cover[comp_all$cover == ">5"] <- 7.5
comp_all$cover[comp_all$cover == "105"] <- 100
comp_all$cover[comp_all$cover == "775"] <- 75  # PBA: I checked this one in the raw data, plant cover at this site = 25
comp_all$cover <- as.numeric(comp_all$cover)
# remove non-plant records
table(comp_all$species[is.na(comp_all$ftypes1)]) # first check NAs in ftypes
comp_all <- subset( comp_all, !is.na(comp_all$ftypes1))
# edit column names to match demography file
names(comp_all)
# edit column names to match demography file
names(comp_all)[names(comp_all)=="sitecode"] <- "SiteCode"
# edit column names to match demography file
names(comp_all)
names(comp_all)[names(comp_all)=="sitecode"] <- "SiteCode"
names(comp_all)[names(comp_all)=="year"] <- "Year"
names(comp_all)[names(comp_all)=="transect"] <- "Transect"
names(comp_all)[names(comp_all)=="treatment"] <- "Treatment"
names(comp_all)[names(comp_all)=="distance_m"] <- "Distance"
# conduct individual level analysis of climate and competition
# effects on cheatgrass fitness in satellite experiments
# start clean
rm(list=ls())
# To use relative paths, we need to set working directory to source file location
# (this method only works on Rstudio)
library(rstudioapi)
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path )) # set working directory to location of this file
# load packages
library(dplyr)
library(tidyr)
library(maps)
# load and clean 2021, 2022, and 2023 demography data
source("load_and_clean_demography_data.R")
# pull Daymet data for each site, multiple years
# this takes a few minutes, no need to run more than once
# source("pull_climate_data.R")
# load site info and site climate data
source("load_site_climate_data.R")
# load and clean composition data, add functional group info
source("composition2functional_groups.R")
names(D)
names(comp_ftypes1)
# merge demography and composition data, check and clean
test <- merge(D,comp_ftypes1,all.x=T)
colSums(is.na(test))
View(test)
sort(unique(D$SiteCode))
sort(unique(comp_all$SiteCode))
sort(unique(comp_ftypes1$SiteCode))
table(test$SiteCode[is.na(test$annual)])
colSums(is.na(comp_ftypes1))
test[test$SiteCode=="EOARC" & is.na(test$annual),]
names(comp_ftypes1)
tmp <- which(comp_ftypes1$SiteCode=="EOARC" & Year == 2022)
tmp <- which(comp_ftypes1$SiteCode=="EOARC" & comp_ftypes1$Year == 2022)
comp_ftypes1[tmp,]
print(n=200)
print(comp_ftypes1,n=200)
print(comp_ftypes1[tmp],n=200)
print(comp_ftypes1[tmp,],n=200)
tmp <- which(D$SiteCode=="EOARC" & D$Year == 2022)
D[tmp,]
# conduct individual level analysis of climate and competition
# effects on cheatgrass fitness in satellite experiments
# start clean
rm(list=ls())
# To use relative paths, we need to set working directory to source file location
# (this method only works on Rstudio)
library(rstudioapi)
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path )) # set working directory to location of this file
# load packages
library(dplyr)
library(tidyr)
library(maps)
# load and clean 2021, 2022, and 2023 demography data
source("load_and_clean_demography_data.R")
# pull Daymet data for each site, multiple years
# this takes a few minutes, no need to run more than once
# source("pull_climate_data.R")
# load site info and site climate data
source("load_site_climate_data.R")
# load and clean composition data, add functional group info
source("composition2functional_groups.R")
test <- merge(D,comp_ftypes1,all=T)
colSums(is.na(test))
table(test$SiteCode[is.na(test$annual)])
View(test)
# figure out why demography and composition data don't match
problems <- test[is.na(test$annual),]
problems <- problems[,c("SiteCode", "Year","Treatment","Transect")]
problems<-unique(problems,margin=2)
for(i in 1:nrow(problems)){
tmp <- which(D$SiteCode==problems$SiteCode[i] &
D$Year==problems$Year[i] &
D$Treatment==problems$Treatment[i] &
D$Transect==problems$Transect[i])
tmpD <- D[tmp,]
tmp <- which(comp_ftypes1$SiteCode==problems$SiteCode[i] &
comp_ftypes1$Year==problems$Year[i] &
comp_ftypes1$Treatment==problems$Treatment[i] &
comp_ftypes1$Transect==problems$Transect[i])
tmpC <- comp_ftypes1[tmp,]
print(tmpD); print(tmpC)
}
i=1
tmp <- which(D$SiteCode==problems$SiteCode[i] &
D$Year==problems$Year[i] &
D$Treatment==problems$Treatment[i] &
D$Transect==problems$Transect[i])
tmpD <- D[tmp,]
tmp <- which(comp_ftypes1$SiteCode==problems$SiteCode[i] &
comp_ftypes1$Year==problems$Year[i] &
comp_ftypes1$Treatment==problems$Treatment[i] &
comp_ftypes1$Transect==problems$Transect[i])
tmpC <- comp_ftypes1[tmp,]
print(tmpD); print(tmpC)
# figure out why demography and composition data don't match
problems <- test[is.na(test$annual),]
problems <- problems[,c("SiteCode", "Year","Treatment","Transect","Distance")]
View(problems)
View(problems)
View(test)
