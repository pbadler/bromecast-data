summarize(cover = sum(cover))
comp_ftypes2 <- as.data.frame(comp_ftypes2)
comp_ftypes2 <- reshape(comp_ftypes2, direction = "wide",
idvar = c("SiteCode","Year","Transect","Treatment","Distance"),
timevar = "ftypes2")
comp_ftypes3 <- comp_all %>% group_by(SiteCode,Year,Transect,Treatment,Distance, ftypes3) %>%
summarize(cover = sum(cover))
comp_ftypes3 <- as.data.frame(comp_ftypes3)
comp_ftypes3 <- reshape(comp_ftypes3, direction = "wide",
idvar = c("SiteCode","Year","Transect","Treatment","Distance"),
timevar = "ftypes3")
brte <- comp_all %>% filter(species=="Bromus tectorum") %>%
group_by(SiteCode,Year,Treatment) %>%
summarize(cover = mean(cover))
View(brte)
View(comp_ftypes1)
tmp <- names(comp_fypes1)
tmp <- names(comp_ftypes1)
gsub("cover.","",tmp)
comp_ftypes1 <- comp_all %>% group_by(SiteCode,Year,Transect,Treatment,Distance, ftypes1) %>%
summarize(cover = sum(cover))
comp_ftypes1 <- as.data.frame(comp_ftypes1)
comp_ftypes1 <- reshape(comp_ftypes1, direction = "wide",
idvar = c("SiteCode","Year","Transect","Treatment","Distance"),
timevar = "ftypes1")
names(comp_ftypes1) <- gsub("cover.","",names(comp_ftypes1))
View(comp_ftypes1)
View(brte)
brte <- comp_all %>% filter(species=="Bromus tectorum") %>%
group_by(SiteCode,Year,Treatment) %>%
summarize(cover = mean(cover))
brte <- as.data.frame(brte)
brte <- reshape(brte, direction = "wide",
idvar = c("SiteCode","Year"),
timevar = "Treatment")
View(brte)
names(brte) <- gsub("cover.","",names(brte))
write.csv(brte,"../deriveddata/brte_cover_siteyeartrt.csv",row.names=F)
rm(brte)
rm(comp_all)
source("C:/repos/bromecast-data/satellites/code/composition2functional_groups.R")
# merge demography site data
D <- merge(D,siteD,all.x=T)
# fix a couple data entry errors
D$Distance[D$SiteCode=="Symstad2" & D$Distance==5.04] <- 5
D$Distance[D$SiteCode=="Symstad2" & D$Distance==5.17] <- 5.2
# fixable problems solved, now do the merge
allD_ft1 <- merge(D,comp_ftypes1,all.x=T)
write.csv(allD_ft1,"../deriveddata/all_plants_ftypes1.csv",row.names=F)
allD_ft2 <- merge(D,comp_ftypes2,all.x=T)
write.csv(allD_ft2,"../deriveddata/all_plants_ftypes2.csv",row.names=F)
allD_ft3 <- merge(D,comp_ftypes3,all.x=T)
write.csv(allD_ft3,"../deriveddata/all_plants_ftypes3.csv",row.names=F)
#cleanup
rm(D, fgroups, comp_ftypes1,comp_ftypes2,comp_ftypes3)
View(allD_ft1)
allD_ft1[allD_ft1$SiteCode=="dino",]
# prepare data for individual level analysis of climate and competition
# effects on cheatgrass fitness in satellite experiments
# start clean
rm(list=ls())
# To use relative paths, we need to set working directory to source file location
# (this method only works on Rstudio)
library(rstudioapi)
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path )) # set working directory to location of this file
# load packages
library(dplyr)
#library(tidyr)
library(maps)
# load and clean 2021, 2022, 2023, and 2024 demography data
source("load_and_clean_demography_data.R")
# load site info and site climate data
source("load_site_climate_data.R")
View(siteD)
# This code
# 1. cleans up and combines compositional data from all years into one composition
# file written to deriveddata,
# 2. compiles and cleans species names for Bromus tectorum "competitors" and
# 3. joins functional type data to species names
# To use relative paths, we need to set working directory to source file location
# (this method only works on Rstudio)
library(rstudioapi)
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path )) # set working directory to location of this file
# Load libraries
#library(tidyverse)
# Make %notin% operator
`%notin%` <- Negate(`%in%`)
# Read in 2020-2021 composition data
comp21 <- read.csv("../rawdata/Satellite_composition_2020-2021.csv")
# Read in 2021-2022 composition data
comp22 <- read.csv("../rawdata/Satellite_composition_2021-2022.csv")
# Read in 2022-2023 composition data
comp23 <- read.csv("../rawdata/Satellite_composition_2022-2023.csv")
# Read in 2022-2023 composition data
comp24 <- read.csv("../rawdata/Satellite_composition_2023-2024.csv")
# Rename columns for brevity
comp21 %>% mutate(year=2021) %>%
select(sitecode = SiteCode,
year=year,
transect = "Transect..N..E..S..or.W.",
treatment = "Treatment..control.OR.removal.",
distance_m = "Distance.from.center..m.",
species = Species,
cover = Cover) -> comp21
#litter_depth_cm = `Litter depth (if >1cm)`,
#notes = Notes) -> comp20
#add missing columns
comp21$litter_depth_cm <- NA
comp21$notes <- NA
comp22 %>% mutate(year=2022) %>%
select(sitecode = SiteCode,
year=year,
transect = "Transect..N..E..S..or.W.",
treatment = "Treatment..control.OR.removal.",
distance_m = "Distance.from.center..m.",
species = Species,
cover = Cover,
litter_depth_cm = "Litter.depth..cm.",
notes = Notes) -> comp22
comp23 %>% mutate(year=2023) %>%
select(sitecode = SiteCode,
year=year,
transect = "Transect..N..E..S..or.W.",
treatment = "Treatment..control.OR.removal.",
distance_m = "Distance.from.center..m.",
species = Species,
cover = Cover,
litter_depth_cm = "Litter.depth..if..1cm." ,
notes = Notes) -> comp23
comp24 %>% mutate(year=2024) %>%
select(sitecode = SiteCode,
year=year,
transect = "Transect..N..E..S..or.W.",
treatment = "Treatment..control.OR.removal.",
distance_m = "Distance.from.center..m.",
species = Species,
cover = Cover,
litter_depth_cm = "Litter.depth..if..1cm." ,
notes = Notes) -> comp24
# Combine species observation lists
comp_all <- rbind(comp21, comp22, comp23, comp24)
comp_all <- comp_all[order(comp_all$sitecode,comp_all$year,comp_all$transect,comp_all$distance_m),]
rm(comp21,comp22,comp23,comp24)
# PBA: I'm ignoring the problematic "notes" because I cleaned the demography
# data very carefully. That catches the real problems. Leftover notes for composition
# mostly concern species ID, which don't matter much at functional group level.
# Write csv file to code species as functional groups manually and to fix any issues
species_list <- sort(unique(comp_all$species))
write.csv(species_list, "../deriveddata/species_list_raw.csv" )
# did a previous species list exist?
tmp <- file.exists("../deriveddata/species_list_updates.csv")
# if so, add any new entries to that list
if(tmp==T){
update_list <- read.csv("../deriveddata/species_list_updates.csv")
update_list <- update_list[,c("species","update")]
tmp <- species_list %notin% update_list$species
if(sum(tmp)>0){
tmp <- data.frame("species" = species_list[tmp], "update" = NA)
update_list <- rbind(update_list,tmp)
update_list <- update_list[order(update_list$species),]
#write to file
write.csv(update_list,"../deriveddata/species_list_updates.csv",row.names=F)
}
}
# DONE BY HAND: fill in "update" column in
# "../deriveddata/species_list_updates.csv" as needed
# # # look up sitecode for a species code
# findspp <- " Symphyotrichum porteri"
# comp_all[comp_all$species==findspp,]
# Read in species updates
updated_names <- read.csv("../deriveddata/species_list_updates.csv",header=T)
updated_names <- updated_names[,c("species","update")] #drop site column
# fill missing values with NA
tmp <- which(updated_names$update=="")
updated_names$update[tmp] <- NA
# update species names
comp_all <- merge(comp_all, updated_names, all.x = T)
tmp <- which(!is.na(comp_all$update))
comp_all$species[tmp] <- comp_all$update[tmp]
comp_all <- dplyr::select(comp_all,-update) # drop update column
# write updated species list (to join with functional trait data)
species_list_clean <- data.frame(species=sort(unique(comp_all$species)))
write.csv(species_list_clean,"../deriveddata/species_list_clean.csv",row.names=F)
# clean up
rm(tmp,update_list,updated_names,species_list_clean)
###
# join species list with functional type data
spp_list <- read.csv("../deriveddata/species_list_clean.csv",header=T)
fgroups <- read.csv("../deriveddata/species2functionalgroups.csv",header=T)
spp_list <- merge(spp_list,fgroups, all.x=T)
write.csv(spp_list,"../deriveddata/species2functionalgroups.csv", row.names=F)
rm(spp_list)
# to do by hand: fill out functional types for any new (missing) species
# read in latest version of functional type data
fgroups <- read.csv("../deriveddata/species2functionalgroups.csv",header=T)
### define functional groups
# annual, perennial, shrub, biocrust, unknown, groundcover
fgroups$ftypes1 <- NA
fgroups$ftypes1[fgroups$type=="non-plant"] <- "groundcover"
fgroups$ftypes1[fgroups$duration=="annual" | fgroups$duration=="biennial"] <- "annual"
fgroups$ftypes1[fgroups$duration=="perennial"] <- "perennial"
fgroups$ftypes1[fgroups$duration=="unknown"] <- "unknown"
fgroups$ftypes1[fgroups$type=="shrub"] <- "shrub"
fgroups$ftypes1[fgroups$type=="biocrust"] <- "biocrust"
# unknowns  NAs, these will be CUT
# annual forb, annual grass, perennial forb, perennial grass, shrub, biocrust, unknown
fgroups$ftypes2 <- fgroups$ftypes1
tmp <- which(fgroups$ftypes1=="annual")
fgroups$ftypes2[tmp] <- paste0(fgroups$ftypes2[tmp],fgroups$type[tmp])
tmp <- which(fgroups$ftypes1=="perennial")
fgroups$ftypes2[tmp] <- paste0(fgroups$ftypes2[tmp],fgroups$type[tmp])
# annual forb, annual grass, perennial forb, perennial grass c3,  perennial grass c4, shrub, biocrust, unknown
fgroups$ftypes3 <- fgroups$ftypes2
tmp <- which(fgroups$ftypes2=="perennialgrass")
fgroups$ftypes3[tmp] <- paste0(fgroups$ftypes3[tmp],fgroups$c3c4[tmp])
fgroups <- fgroups[,c("species","ftypes1","ftypes2","ftypes3")]
### join functional group data to composition data and do some cleaning
comp_all <- merge(comp_all,fgroups,all.x=T)
# remove records where cover == NA. These are missing toothpicks and 2 bare ground obs
tmp <- which(is.na(comp_all$cover))
comp_all <- comp_all[-tmp,]
# remove other missing records flagged by value of cover = -1
tmp <- which(comp_all$cover == -1)
comp_all <- comp_all[-tmp,]
tmp <- which(comp_all$cover == "missing")
comp_all <- comp_all[-tmp,]
tmp <- which(comp_all$cover == "M")
comp_all <- comp_all[-tmp,]
tmp <- which(comp_all$species == "cut_this_record")
comp_all <- comp_all[-tmp,]
tmp <- which(comp_all$species == "missing")
comp_all <- comp_all[-tmp,]
# other checks
table(comp_all$ftypes1)
table(comp_all$cover)
# fix non-numeric cover values
comp_all$cover[comp_all$cover == "<1"] <- 0.5
comp_all$cover[comp_all$cover == ">5"] <- 7.5
comp_all$cover[comp_all$cover == "105"] <- 100
comp_all$cover[comp_all$cover == "775"] <- 75  # PBA: I checked this one in the raw data, plant cover at this site = 25
comp_all$cover <- as.numeric(comp_all$cover)
# remove non-plant records
table(comp_all$species[is.na(comp_all$ftypes1)]) # first check NAs in ftypes
comp_all <- subset( comp_all, !is.na(comp_all$ftypes1))
# check NOTES? PBA: I don't see high priority problems here
# edit column names to match demography file
names(comp_all)[names(comp_all)=="sitecode"] <- "SiteCode"
names(comp_all)[names(comp_all)=="year"] <- "Year"
names(comp_all)[names(comp_all)=="transect"] <- "Transect"
names(comp_all)[names(comp_all)=="treatment"] <- "Treatment"
names(comp_all)[names(comp_all)=="distance_m"] <- "Distance"
# clean up bad Treatment values
comp_all$Treatment[comp_all$Treatment == "Control"] <- "control"
comp_all$Treatment[comp_all$Treatment == "Removal"] <- "removal"
comp_all$Treatment[comp_all$Treatment == "removal "] <- "removal"
tmp <- which(comp_all$SiteCode=="EnsingS1" & comp_all$Year==2022 & comp_all$Transect=="W")
comp_all$Treatment[tmp] <- "removal"
tmp <- which(comp_all$SiteCode=="EnsingS1" & comp_all$Year==2022 & comp_all$Transect=="E")
comp_all$Treatment[tmp] <- "removal"
# clean up bad distances
tmp <- which(comp_all$SiteCode=="EOARC" & comp_all$Year==2022 & comp_all$Transect=="E")
comp_all$Distance[tmp] <- comp_all$Distance[tmp] + 2
tmp <- which(comp_all$SiteCode=="EOARC" & comp_all$Year==2022 & comp_all$Transect=="S")
comp_all$Distance[tmp] <- comp_all$Distance[tmp] + 2
# make sure site names match demography file
comp_all$SiteCode[comp_all$SiteCode=="EnsingS1"] <- "EnsingS1 SuRDC"
comp_all$SiteCode[comp_all$SiteCode=="EnsingS2"] <- "EnsingS2 Summerland-Princeton"
comp_all$SiteCode[comp_all$SiteCode=="EnsingS3"] <- "EnsingS3 Bear Creek"
comp_all$SiteCode[comp_all$SiteCode=="EnsingS4"] <- "EnsingS4 Lundbom"
comp_all$SiteCode[comp_all$SiteCode=="SymstadS1"] <- "Symstad1"
comp_all$SiteCode[comp_all$SiteCode=="South Eden"] <- "SouthEden"
comp_all$SiteCode[comp_all$SiteCode=="Hardware"] <- "HardwareRanch"
comp_ftypes1 <- comp_all %>% group_by(SiteCode,Year,Transect,Treatment,Distance, ftypes1) %>%
summarize(cover = sum(cover))
comp_ftypes1 <- as.data.frame(comp_ftypes1)
comp_ftypes1 <- reshape(comp_ftypes1, direction = "wide",
idvar = c("SiteCode","Year","Transect","Treatment","Distance"),
timevar = "ftypes1")
names(comp_ftypes1) <- gsub("cover.","",names(comp_ftypes1))
View(comp_ftypes1)
# replace NA cover values with zeros
comp_ftypes1[is.na(comp_ftypes1)] <- NA
View(comp_ftypes1)
# replace NA cover values with zeros
comp_ftypes1[is.na(comp_ftypes1)] <- 0
View(comp_ftypes1)
source("C:/repos/bromecast-data/satellites/code/run_everything.R")
# prepare data for individual level analysis of climate and competition
# effects on cheatgrass fitness in satellite experiments
# start clean
rm(list=ls())
# To use relative paths, we need to set working directory to source file location
# (this method only works on Rstudio)
library(rstudioapi)
current_path <- getActiveDocumentContext()$path
setwd(dirname(current_path )) # set working directory to location of this file
# load packages
library(dplyr)
#library(tidyr)
library(maps)
# load and clean 2021, 2022, 2023, and 2024 demography data
source("load_and_clean_demography_data.R")
# year 2021
siteD1 <- read.csv("../rawdata/SiteInfo_2020-2021.csv",header=T)
names(siteD1)[1:3] <- c("SiteCode" ,"Lat","Lon")
siteD1$Year = 2021
# make site SiteCodes match those in the demography file
siteD1$SiteCode[siteD1$SiteCode=="SymstadS1"] <- "Symstad1"
# year 2022
siteD2 <- read.csv("../rawdata/SiteInfo_2021-2022.csv",header=T)
names(siteD2)[1:3] <- c("SiteCode" ,"Lat","Lon")
siteD2$Year = 2022
# make site SiteCodes match those in the demography file
siteD2$SiteCode[siteD2$SiteCode=="EnsingS1_SuRDC"] <- "EnsingS1 SuRDC"
siteD2$SiteCode[siteD2$SiteCode=="EnsingS2_SumPrinceRd"] <- "EnsingS2 Summerland-Princeton"
siteD2$SiteCode[siteD2$SiteCode=="EnsingS3_BearCreek"] <- "EnsingS3 Bear Creek"
siteD2$SiteCode[siteD2$SiteCode=="EnsingS4_LDBM"] <- "EnsingS4 Lundbom"
siteD2$SiteCode[siteD2$SiteCode=="SymstadS1"] <- "Symstad1"
siteD2$SiteCode[siteD2$SiteCode=="SymstadS2"] <- "Symstad2"
# year 2023
siteD3 <- read.csv("../rawdata/SiteInfo_2022-2023.csv",header=T)
names(siteD3)[1:3] <- c("SiteCode" ,"Lat","Lon")
siteD3$Year = 2023
# year 2024
siteD4 <- read.csv("../rawdata/SiteInfo_2023-2024.csv",header=T)
names(siteD4)[1:3] <- c("SiteCode" ,"Lat","Lon")
siteD4$Year = 2024
# combine years into one data frame
siteD <- rbind(siteD1[,c("SiteCode","Lat","Lon","Year")],
siteD2[,c("SiteCode","Lat","Lon","Year")],
siteD3[,c("SiteCode","Lat","Lon","Year")],
siteD4[,c("SiteCode","Lat","Lon","Year")])
#remove Lehnoff sites
tmp <- grep("LEHN",siteD$SiteCode)
siteD <- siteD[-tmp,]
rm(siteD1,siteD2, siteD3, siteD4, tmp)
# in case daymet daily data already acquired, load here
climD <- read.csv(climD,"../deriveddata/Satellites_daymet_daily.csv",header=T)
# in case daymet daily data already acquired, load here
climD <- read.csv("../deriveddata/Satellites_daymet_daily.csv",header=T)
View(climD)
# only consider fall - spring
Fa2SprD <- subset(climD,climD$climDay < 270)
View(climD)
# only consider fall - spring
Fa2SprD <- subset(climD,climD$climDay < 270)
# set up climate seasons
Fa2SprD$season <- "Win"
Fa2SprD$season[Fa2SprD$climDay < 93] <- "Fall"
Fa2SprD$season[Fa2SprD$climDay > 184] <- "Spr"
View(climD)
View(Fa2SprD)
# only consider fall - spring
Fa2SprD <- subset(climD,climD$climDay < 270)
# set up climate seasons
Fa2SprD$season <- "Win"
Fa2SprD$season[Fa2SprD$climDay < 92] <- "Fall"
Fa2SprD$season[Fa2SprD$climDay > 184] <- "Spr"
# calculate daily mean temperature
Fa2SprD$tavg <- (Fa2SprD$tmax + Fa2SprD$tmin)/2
annD <- Fa2SprD %>% group_by(SiteCode,climYr,season) %>%
summarise(prcp=sum(prcp),
tmean=mean(tavg),
swe_mean=mean(swe),
swe_days=sum(swe>0))
View(annD)
?reshape
annD <- as.data.frame(annD)
test <- reshape(annD, direction="wide",
idvar=c("SiteCode","climYr"),
timvar="season"
)
test <- reshape(annD, direction="wide",
idvar=c("SiteCode","climYr"),
timevar="season"
)
View(test)
annD <- Fa2SprD %>% group_by(SiteCode,climYr,season) %>%
summarise(prcp=sum(prcp),
tmean=mean(tavg),
swe_mean=mean(swe)) #,
View(annD)
#swe_days=sum(swe>0))
annD <- as.data.frame(annD)
annD <- reshape(annD, direction="wide",
idvar=c("SiteCode","climYr"),
timevar="season" )
View(annD)
View(annD)
# since climYr 2024 data is incomplete (only fall 2023 observations available)
# set climYr 2024 Win and Spr values to NA
tmp <- grep(".Spr",names(annD))
tmp
names(annD)[tmp]
# since climYr 2024 data is incomplete (only fall 2023 observations available)
# set climYr 2024 Win and Spr values to NA
tmp <- grep(".Win",names(annD))
annD[annD$climYr==2024,tmp] <- NA
View(annD)
# save annual data to file
write.csv(annD,"../deriveddata/Satellites_daymet_Fall2Spr_means.csv",row.names=F)
rm(climD,Fa2SprD,annD,siteD,tmp)
# in case daymet daily data already acquired, load here
climD <- read.csv("../deriveddata/Satellites_daymet_daily.csv",header=T)
# only consider fall - spring
Fa2SprD <- subset(climD,climD$climDay < 270)
# set up climate seasons
Fa2SprD$season <- "Win"
Fa2SprD$season[Fa2SprD$climDay < 92] <- "Fall"
Fa2SprD$season[Fa2SprD$climDay > 184] <- "Spr"
# calculate daily mean temperature
Fa2SprD$tavg <- (Fa2SprD$tmax + Fa2SprD$tmin)/2
annD <- Fa2SprD %>% group_by(SiteCode,climYr,season) %>%
summarise(prcp=sum(prcp),
tmean=mean(tavg),
swe_mean=mean(swe)) #,
#swe_days=sum(swe>0))
annD <- as.data.frame(annD)
annD <- reshape(annD, direction="wide",
idvar=c("SiteCode","climYr"),
timevar="season" )
# since climYr 2024 data is incomplete (only fall 2023 observations available)
# set climYr 2024 Win and Spr values to NA
tmp <- grep(".Win",names(annD))
annD[annD$climYr==2024,tmp] <- NA
tmp <- grep(".Spr",names(annD))
annD[annD$climYr==2024,tmp] <- NA
# save annual data to file
write.csv(annD,"../deriveddata/Satellites_daymet_Fall2Spr_means.csv",row.names=F)
rm(climD,Fa2SprD,annD,siteD,tmp)
# in case daymet daily data already acquired, load here
climD <- read.csv("../deriveddata/Satellites_daymet_daily.csv",header=T)
# only consider fall - spring
Fa2SprD <- subset(climD,climD$climDay < 270)
# set up climate seasons
Fa2SprD$season <- "Win"
Fa2SprD$season[Fa2SprD$climDay < 92] <- "Fall"
Fa2SprD$season[Fa2SprD$climDay > 184] <- "Spr"
# calculate daily mean temperature
Fa2SprD$tavg <- (Fa2SprD$tmax + Fa2SprD$tmin)/2
annD <- Fa2SprD %>% group_by(SiteCode,climYr,season) %>%
summarise(prcp=sum(prcp),
tmean=mean(tavg),
swe_mean=mean(swe)) #,
#swe_days=sum(swe>0))
annD <- as.data.frame(annD)
annD <- reshape(annD, direction="wide",
idvar=c("SiteCode","climYr"),
timevar="season" )
# since climYr 2024 data is incomplete (only fall 2023 observations available)
# set climYr 2024 Win and Spr values to NA
tmp <- grep(".Win",names(annD))
annD[annD$climYr==2024,tmp] <- NA
tmp <- grep(".Spr",names(annD))
annD[annD$climYr==2024,tmp] <- NA
# save annual data to file
write.csv(annD,"../deriveddata/Satellites_daymet_Fall2Spr_means.csv",row.names=F)
rm(climD,Fa2SprD,annD,siteD,tmp)
# import lat lons
siteD <- read.csv("../rawdata/SiteInfo_2021-2022.csv",header=T)
siteD <- siteD[,1:3]
names(siteD) <- c("SiteCode" ,"Lat","Lon")
tmp <- read.csv("../rawdata/SiteInfo_2020-2021.csv",header=T)
tmp <- tmp[,1:3]
names(tmp) <- c("SiteCode" ,"Lat","Lon")
siteD <- rbind(siteD,tmp)
tmp <- read.csv("../rawdata/SiteInfo_2022-2023.csv",header=T)
tmp <- tmp[,1:3]
names(tmp) <- c("SiteCode" ,"Lat","Lon")
siteD <- rbind(siteD,tmp)
tmp <- read.csv("../rawdata/SiteInfo_2023-2024.csv",header=T)
tmp <- tmp[,1:3]
names(tmp) <- c("SiteCode" ,"Lat","Lon")
siteD <- rbind(siteD,tmp)
# remove duplicates
siteD <- unique(siteD, MARGIN=2)
tmp <- which(siteD$SiteCode=="SymstadS1" & siteD$Lat==43.35620) # remove SymstadS1 w/ bad coords
siteD <- siteD[-tmp,]
# drop Lehnoff sites (no data)
tmp <- grep("LEHN", siteD$SiteCode)
siteD <- siteD[-tmp,]
# in case daymet daily data already acquired, load here
climD <- read.csv("../deriveddata/Satellites_daymet_daily.csv",header=T)
# only consider fall - spring
Fa2SprD <- subset(climD,climD$climDay < 270)
# set up climate seasons
Fa2SprD$season <- "Win"
Fa2SprD$season[Fa2SprD$climDay < 92] <- "Fall"
Fa2SprD$season[Fa2SprD$climDay > 184] <- "Spr"
# calculate daily mean temperature
Fa2SprD$tavg <- (Fa2SprD$tmax + Fa2SprD$tmin)/2
annD <- Fa2SprD %>% group_by(SiteCode,climYr,season) %>%
summarise(prcp=sum(prcp),
tmean=mean(tavg),
swe_mean=mean(swe)) #,
#swe_days=sum(swe>0))
annD <- as.data.frame(annD)
annD <- reshape(annD, direction="wide",
idvar=c("SiteCode","climYr"),
timevar="season" )
# since climYr 2024 data is incomplete (only fall 2023 observations available)
# set climYr 2024 Win and Spr values to NA
tmp <- grep(".Win",names(annD))
annD[annD$climYr==2024,tmp] <- NA
tmp <- grep(".Spr",names(annD))
annD[annD$climYr==2024,tmp] <- NA
# save annual data to file
write.csv(annD,"../deriveddata/Satellites_daymet_Fall2Spr_means.csv",row.names=F)
rm(climD,Fa2SprD,annD,siteD,tmp)
source("C:/repos/bromecast-data/satellites/code/run_everything.R")
